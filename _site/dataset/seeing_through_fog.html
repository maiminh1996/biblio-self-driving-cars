<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>biblio-self-driving-cars</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="biblio-self-driving-cars" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/dataset/seeing_through_fog.html" />
<meta property="og:url" content="http://localhost:4000/dataset/seeing_through_fog.html" />
<meta property="og:site_name" content="biblio-self-driving-cars" />
<script type="application/ld+json">
{"@type":"WebPage","url":"http://localhost:4000/dataset/seeing_through_fog.html","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://maiminh1996.github.io/minh.png"}},"headline":"biblio-self-driving-cars","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=e8f795b6dadcc381d57ecd565d6b297a714a47a7">
    <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="http://localhost:4000/">biblio-self-driving-cars</a></h1>
        
        
          <img src="https://maiminh1996.github.io/minh.png" alt="Logo" />
        

        <p></p>

        
        <p class="view"><a href="https://github.com/maiminh1996/biblio-self-driving-cars">View the Project on GitHub <small>maiminh1996/biblio-self-driving-cars</small></a></p>
        

        

        
      </header>
      <section>

      <!-- CSS -->
<link rel="stylesheet" style="text/css" href="../styles.css" />

<!--     -->

<h2 id="seeing-through-fog-">Seeing Through Fog <img src="../doc/0.png" width="95" /></h2>
<h3 id="cvpr-20-seeing-through-fog-without-seeing-fog-deep-multimodal-sensor-fusion-in-unseen-adverse-weather"><a href="https://www.cs.princeton.edu/~fheide/AdverseWeatherFusion/"><kbd>CVPR 20</kbd> Seeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather</a></h3>

<h3 id="iccv-19-gated2depth-real-time-dense-lidar-from-gated-images"><a href="https://arxiv.org/pdf/1902.04997.pdf"><kbd>ICCV 19</kbd> Gated2Depth: Real-time Dense Lidar from Gated Images</a></h3>

<p><a href="https://www.cs.princeton.edu/~fheide/AdverseWeatherFusion/">Link to web project</a></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Category</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">Overall impression</td>
      <td>a  multimodal  adverse  weather  datasetcovering camera, lidar, radar, gated NIR, and FIR sensor data which contains rare scenarios, such asheavy fog, heavy snow, and severe rain, during morethan 10,000 km of driving in northern Europe.</td>
    </tr>
    <tr>
      <td style="text-align: center">Inputs</td>
      <td>● Stereo (1920x1024) <br /> ● Gated cam (1280x720) <br /> ● RADAR <br /> ● LiDAR: 1 HDL64 S3D and VLP32C <br /> ● FIR cam<br /> All  sensors  are  time-synchronized and ego-motion corrected using a proprietaryinertial  measurement  unit  (IMU).  The  system  provides  asampling rate of 10 Hz</td>
    </tr>
    <tr>
      <td style="text-align: center">Scenes</td>
      <td>2 test drives in Feb and Dec 2019 in Germany, Sweden, Denmark, and Finland for twoweeks  each,  covering  a  distance  of  10,000 km  under  different weather and illumination conditions.  A total of 1.4million frames at a frame rate of 10 Hz have been collected</td>
    </tr>
    <tr>
      <td style="text-align: center">Condition</td>
      <td>clear weather, dense fog, inlight fog, snow/ rain</td>
    </tr>
    <tr>
      <td style="text-align: center">Tasks</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Gth</td>
      <td>Every 100th frame was manually labeled to balance scenetype coverage. The resulting annotations contain 5,5k clear weather  frames,  1k  captures  in  dense  fog,  1k  captures  inlight fog, and 4k captures in snow/rain</td>
    </tr>
    <tr>
      <td style="text-align: center">Evaluation</td>
      <td> </td>
    </tr>
    <tr>
      <td style="text-align: center">Notes</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h2 id="simulation">Simulation</h2>
<p>Simulated data to the clear training data as an alternative approach to tackle the rare harsh weather conditions. <br />
Ref: <a href="https://www.cs.princeton.edu/~fheide/AdverseWeatherFusion/figures/AdverseWeatherFusion_Supplement.pdf">Seeing Through Fog Without Seeing Fog:Deep Multimodal Sensor Fusion in Unseen Adverse Weather(Supplemental Material)</a></p>


      </section>
      <footer>
        
        <p>This project is maintained by <a href="https://github.com/maiminh1996">maiminh1996</a></p>
        
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>
